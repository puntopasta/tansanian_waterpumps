{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Model with PCA\n",
    "In this notebook we transform categorical features to one-hot encoding and then perform PCA (so-called matrix factorisation) to come up with meaningful numeric representation of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib notebook\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_model(test_x,test_y, rfc):\n",
    "    pred = rfc.predict(test_x)\n",
    "    from sklearn.metrics import confusion_matrix, accuracy_score, cohen_kappa_score\n",
    "\n",
    "    print(confusion_matrix(test_y,pred))\n",
    "    print('Acc: ',accuracy_score(test_y,pred))\n",
    "    print('Kappa: ',cohen_kappa_score(test_y,pred))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_categorical(y, num_classes=None):\n",
    "    \"\"\"Converts a class vector (integers) to binary class matrix.\n",
    "\n",
    "    E.g. for use with categorical_crossentropy.\n",
    "\n",
    "    # Arguments\n",
    "        y: class vector to be converted into a matrix\n",
    "            (integers from 0 to num_classes).\n",
    "        num_classes: total number of classes.\n",
    "\n",
    "    # Returns\n",
    "        A binary matrix representation of the input.\n",
    "    \"\"\"\n",
    "    y = np.array(y, dtype='int').ravel()\n",
    "    if not num_classes:\n",
    "        num_classes = np.max(y) + 1\n",
    "    n = y.shape[0]\n",
    "    categorical = np.zeros((n, num_classes))\n",
    "    categorical[np.arange(n), y] = 1\n",
    "    return categorical\n",
    "\n",
    "def categorical_factorisation(data, test_data):\n",
    "    '''\n",
    "    User matrix factorisation to arrive at numerical representation that captures most variance.\n",
    "    :param data: data\n",
    "    :return: data, with principal component representation of each categorical feature.\n",
    "    '''\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    for feature in data.columns:\n",
    "        if data[feature].dtype == 'O':\n",
    "            le = preprocessing.LabelEncoder()\n",
    "            le.fit(data[feature])\n",
    "            num = le.transform(data[feature])\n",
    "            one_hot = to_categorical(num)\n",
    "            arity = one_hot.shape[-1]\n",
    "            if arity > 100:\n",
    "                continue\n",
    "            max_components = int(np.min([10, np.ceil(arity/2)]))\n",
    "            pca = PCA(n_components=max_components)\n",
    "            components = pca.fit_transform(one_hot)\n",
    "            component_names = ['{f}_{i}'.format(f=feature, i=i) for i in range(max_components)]\n",
    "            new_features = pd.DataFrame(data=components, columns=component_names, index=data.index)\n",
    "            data = pd.concat([data,new_features],axis=1)\n",
    "            \n",
    "            # now do the same for the test data, but re-apply the components from training.\n",
    "            num = le.transform(test_data[feature])\n",
    "            one_hot = to_categorical(num)\n",
    "            components = pca.transform(one_hot)\n",
    "            new_features = pd.DataFrame(data=components, columns=component_names, index=test_data.index)\n",
    "            test_data = pd.concat([test_data,new_features],axis=1)\n",
    "    return data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features = [\n",
    "    'gps_height',\n",
    "    'latitude',\n",
    "    'longitude',\n",
    "    'population',\n",
    "    'amount_tsh',\n",
    "    'age_at_measurement',\n",
    "    'payment_type',\n",
    "    'management_group',\n",
    "    'quality_group',\n",
    "    'region',\n",
    "    'basin',\n",
    "    'extraction_type_class',\n",
    "    'quantity_group',\n",
    "    'waterpoint_type_group',\n",
    "    'source_type',\n",
    "    'source_class'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in training set:  Counter({0: 23519, 2: 16750, 1: 2922})\n",
      "Label distribution in testing set:  Counter({0: 7870, 2: 5518, 1: 1009})\n"
     ]
    }
   ],
   "source": [
    "from data_loading import data_loading_pipeline\n",
    "train_df, test_df = data_loading_pipeline('../data', selected_features = None)\n",
    "\n",
    "train_df, test_df = categorical_factorisation(train_df, test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_numeric_features = [c for c in train_df.columns if (train_df[c].dtype != 'O') and c != 'status_group']\n",
    "\n",
    "train_x = train_df[all_numeric_features]\n",
    "test_x = test_df[all_numeric_features]\n",
    "\n",
    "train_y = train_df.status_group.as_matrix()\n",
    "test_y = test_df.status_group.as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use random forests to optimise the feature set using cross-validated recursive feature elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = RandomForestClassifier(n_estimators=100)\n",
    "selector = RFE(estimator=algo)\n",
    "\n",
    "selector.fit(train_x, train_y)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "print('Selected features: ')\n",
    "print(np.array(all_numeric_features)[[s for s in selector.get_support()]])\n",
    "print('\\n\\nDiscarded features: ')\n",
    "print(np.array(all_numeric_features)[[not s for s in selector.get_support()]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x[train_x.columns[selector.get_support()]]\n",
    "test_x = test_x[test_x.columns[selector.get_support()]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit a large random forest classifier on selected features and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "initial_model = RandomForestClassifier(n_estimators=1000, n_jobs=4)\n",
    "initial_model.fit(train_x, train_y)\n",
    "eval_model(test_x,test_y, initial_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing for minority class by automatically adjusting  class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rfc = RandomForestClassifier(n_estimators=1000, n_jobs=4,class_weight='balanced')\n",
    "rfc.fit(train_x, train_y)\n",
    "eval_model(test_x,test_y, rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersampling the majority class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import ClusterCentroids\n",
    "cc = ClusterCentroids(random_state=0)\n",
    "X_resampled, y_resampled = cc.fit_sample(train_x, train_y)\n",
    "\n",
    "np.random.seed(42)\n",
    "rfc = RandomForestClassifier(n_estimators=1000, n_jobs=4)\n",
    "\n",
    "rfc.fit(X_resampled, y_resampled)\n",
    "eval_model(test_x,test_y, rfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minority class over-sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(n_jobs=4,k=10)\n",
    "train_x_smote, train_y_smote = smote.fit_sample(train_x,train_y)\n",
    "\n",
    "np.random.seed(42)\n",
    "rfc = RandomForestClassifier(n_estimators=1000, n_jobs=4)\n",
    "\n",
    "rfc.fit(train_x_smote, train_y_smote)\n",
    "eval_model(test_x,test_y, rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
